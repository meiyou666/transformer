import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç  - Transformerçš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€"""
    def __init__(self, d_model, max_seq_length=5000):
        super().__init__()
        
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
        
        # è®¡ç®—ä½ç½®ç¼–ç ï¼šPE(pos, 2i) = sin(pos/10000^(2i/d_model))
        # PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä½ç½®ç”¨sin
        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä½ç½®ç”¨cos
        
        pe = pe.unsqueeze(0).transpose(0, 1)  # [max_seq_length, 1, d_model]
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        # x shape: [seq_length, batch_size, d_model]
        return x + self.pe[:x.size(0), :]

class MultiHeadAttention(nn.Module):
    """å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ - Transformerçš„æ ¸å¿ƒ"""
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # çº¿æ€§å˜æ¢å±‚ï¼šQ, K, V
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(0.1)
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶"""
        # Q, K, V shape: [batch_size, num_heads, seq_length, d_k]
        d_k = Q.size(-1)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼šQ * K^T / sqrt(d_k)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        
        # åº”ç”¨æ©ç ï¼ˆç”¨äºç”Ÿæˆä»»åŠ¡ï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡åˆ°V
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 1. çº¿æ€§å˜æ¢å¾—åˆ°Q, K, V
        Q = self.W_q(query)  # [batch_size, seq_length, d_model]
        K = self.W_k(key)
        V = self.W_v(value)
        
        # 2. é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        # shape: [batch_size, num_heads, seq_length, d_k]
        
        # 3. è®¡ç®—æ³¨æ„åŠ›
        attention_output, attention_weights = self.scaled_dot_product_attention(
            Q, K, V, mask)
        
        # 4. è¿æ¥å¤šå¤´ç»“æœ
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model)
        
        # 5. æœ€ç»ˆçº¿æ€§å˜æ¢
        output = self.W_o(attention_output)
        
        return output, attention_weights

class FeedForward(nn.Module):
    """å‰é¦ˆç¥ç»ç½‘ç»œ - Transformer Blockçš„ç¬¬äºŒä¸ªç»„ä»¶"""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # FFN(x) = max(0, xW1 + b1)W2 + b2
        return self.linear2(self.dropout(F.relu(self.linear1(x))))

class TransformerBlock(nn.Module):
    """Transformerç¼–ç å™¨å—"""
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # 1. å¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        attn_output, attention_weights = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # 2. å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x, attention_weights

class SimpleTransformerLM(nn.Module):
    """ç®€å•çš„Transformerè¯­è¨€æ¨¡å‹ï¼ˆç”Ÿæˆå¼ï¼‰"""
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_seq_length)
        
        # Transformerå—å †å 
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.ln_f = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)
        
        self.dropout = nn.Dropout(dropout)
        
        # å‚æ•°åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def create_causal_mask(self, seq_length):
        """åˆ›å»ºå› æœæ©ç ï¼Œé˜²æ­¢æ¨¡å‹çœ‹åˆ°æœªæ¥çš„token"""
        mask = torch.tril(torch.ones(seq_length, seq_length))
        return mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_length, seq_length]
    
    def forward(self, input_ids, targets=None):
        batch_size, seq_length = input_ids.shape
        
        # 1. è¯åµŒå…¥ + ä½ç½®ç¼–ç 
        # æ³¨æ„ï¼šåµŒå…¥éœ€è¦ç¼©æ”¾
        token_embeddings = self.embedding(input_ids) * math.sqrt(self.d_model)
        token_embeddings = token_embeddings.transpose(0, 1)  # [seq_length, batch_size, d_model]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = self.pos_encoding(token_embeddings)
        x = x.transpose(0, 1)  # [batch_size, seq_length, d_model]
        x = self.dropout(x)
        
        # 2. åˆ›å»ºå› æœæ©ç 
        causal_mask = self.create_causal_mask(seq_length).to(input_ids.device)
        
        # 3. é€šè¿‡Transformerå—
        attention_weights = []
        for transformer_block in self.transformer_blocks:
            x, attn_weights = transformer_block(x, causal_mask)
            attention_weights.append(attn_weights)
        
        # 4. æœ€ç»ˆå±‚å½’ä¸€åŒ– + è¾“å‡ºæŠ•å½±
        x = self.ln_f(x)
        logits = self.head(x)  # [batch_size, seq_length, vocab_size]
        
        loss = None
        if targets is not None:
            # è®¡ç®—äº¤å‰ç†µæŸå¤±
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)), 
                targets.view(-1), 
                ignore_index=-100
            )
        
        return logits, loss, attention_weights
    
    def generate(self, input_ids, max_new_tokens=10, temperature=1.0, top_k=None):
        """æ–‡æœ¬ç”Ÿæˆå‡½æ•°"""
        self.eval()
        
        for _ in range(max_new_tokens):
            # è·å–å½“å‰åºåˆ—çš„logits
            with torch.no_grad():
                logits, _, _ = self(input_ids)
                
                # åªå…³æ³¨æœ€åä¸€ä¸ªä½ç½®çš„logits
                logits = logits[:, -1, :] / temperature  # [batch_size, vocab_size]
                
                # Top-ké‡‡æ ·
                if top_k is not None:
                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                    logits[logits < v[:, [-1]]] = -float('Inf')
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                probs = F.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—
                input_ids = torch.cat([input_ids, next_token], dim=1)
        
        return input_ids

def demo_transformer():
    """æ¼”ç¤ºTransformeræ¨¡å‹"""
    print("ğŸš€ Transformerè¯­è¨€æ¨¡å‹æ¼”ç¤º")
    print("=" * 50)
    
    # æ¨¡å‹å‚æ•°
    vocab_size = 1000      # è¯æ±‡è¡¨å¤§å°
    d_model = 128          # æ¨¡å‹ç»´åº¦
    num_heads = 8          # æ³¨æ„åŠ›å¤´æ•°
    num_layers = 4         # Transformerå±‚æ•°
    d_ff = 512            # å‰é¦ˆç½‘ç»œç»´åº¦
    max_seq_length = 100   # æœ€å¤§åºåˆ—é•¿åº¦
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleTransformerLM(
        vocab_size=vocab_size,
        d_model=d_model,
        num_heads=num_heads,
        num_layers=num_layers,
        d_ff=d_ff,
        max_seq_length=max_seq_length
    )
    
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f} MB")
    
    # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    batch_size = 2
    seq_length = 10
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_length))
    targets = torch.randint(0, vocab_size, (batch_size, seq_length))
    
    print(f"\nğŸ“ è¾“å…¥æ•°æ®:")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  åºåˆ—é•¿åº¦: {seq_length}")
    print(f"  è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    
    # å‰å‘ä¼ æ’­
    print(f"\nâš¡ å‰å‘ä¼ æ’­:")
    logits, loss, attention_weights = model(input_ids, targets)
    
    print(f"  è¾“å‡ºlogitså½¢çŠ¶: {logits.shape}")
    print(f"  æŸå¤±å€¼: {loss.item():.4f}")
    print(f"  æ³¨æ„åŠ›æƒé‡å±‚æ•°: {len(attention_weights)}")
    print(f"  æ¯å±‚æ³¨æ„åŠ›å½¢çŠ¶: {attention_weights[0].shape}")
    
    # æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º
    print(f"\nğŸ¯ æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º:")
    seed_sequence = torch.randint(0, vocab_size, (1, 5))  # ç§å­åºåˆ—
    print(f"  ç§å­åºåˆ—: {seed_sequence.tolist()[0]}")
    
    # ç”Ÿæˆæ–‡æœ¬
    generated = model.generate(
        seed_sequence, 
        max_new_tokens=8, 
        temperature=0.8, 
        top_k=50
    )
    
    print(f"  ç”Ÿæˆåºåˆ—: {generated.tolist()[0]}")
    print(f"  æ–°ç”Ÿæˆçš„tokens: {generated.tolist()[0][5:]}")
    
    # æ³¨æ„åŠ›æƒé‡åˆ†æ
    print(f"\nğŸ” æ³¨æ„åŠ›æƒé‡åˆ†æ (ç¬¬ä¸€å±‚, ç¬¬ä¸€ä¸ªå¤´):")
    first_layer_attn = attention_weights[0][0, 0]  # [seq_length, seq_length]
    print(f"  æ³¨æ„åŠ›çŸ©é˜µå½¢çŠ¶: {first_layer_attn.shape}")
    print(f"  ç¬¬ä¸€è¡Œæ³¨æ„åŠ›æƒé‡ (å‰5ä¸ª): {first_layer_attn[0][:5].tolist()}")
    
    return model

def visualize_attention_pattern():
    """å¯è§†åŒ–æ³¨æ„åŠ›æ¨¡å¼"""
    print(f"\nğŸ¨ æ³¨æ„åŠ›æ¨¡å¼å¯è§†åŒ–:")
    print("-" * 30)
    
    # åˆ›å»ºç®€å•çš„åºåˆ—
    seq = ["I", "love", "machine", "learning", "very", "much"]
    print(f"ç¤ºä¾‹åºåˆ—: {' '.join(seq)}")
    
    # æ¨¡æ‹Ÿæ³¨æ„åŠ›æƒé‡çŸ©é˜µ
    seq_len = len(seq)
    attention_matrix = torch.rand(seq_len, seq_len)
    attention_matrix = F.softmax(attention_matrix, dim=-1)
    
    print(f"\næ³¨æ„åŠ›æƒé‡çŸ©é˜µ:")
    print("    " + "".join(f"{word:>8}" for word in seq))
    for i, word in enumerate(seq):
        weights_str = "".join(f"{attention_matrix[i][j]:>8.3f}" for j in range(seq_len))
        print(f"{word:>4}: {weights_str}")

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    model = demo_transformer()
    
    # å¯è§†åŒ–æ³¨æ„åŠ›æ¨¡å¼
    visualize_attention_pattern()
    
    print(f"\nâœ… Transformeræ¨¡å‹æ¼”ç¤ºå®Œæˆ!")
    print(f"\nğŸ”§ ä¸»è¦ç»„ä»¶è¯´æ˜:")
    print(f"  1. PositionalEncoding: ä½ç½®ç¼–ç ï¼Œè®©æ¨¡å‹ç†è§£åºåˆ—é¡ºåº")
    print(f"  2. MultiHeadAttention: å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•æ‰é•¿è·ç¦»ä¾èµ–")
    print(f"  3. FeedForward: å‰é¦ˆç½‘ç»œï¼Œå¢åŠ æ¨¡å‹éçº¿æ€§è¡¨è¾¾èƒ½åŠ›")
    print(f"  4. TransformerBlock: å®Œæ•´çš„Transformerå—")
    print(f"  5. SimpleTransformerLM: å®Œæ•´çš„ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹")
